import requests
import json
from datetime import datetime
import os
import random
from typing import List, Dict, Any, Optional,Generator,Tuple
from collections import defaultdict
import time

import re 
from config import claude_key,openai_key,tavily_key


def query_gpt_model(prompt: str, article: str, api_key: str=claude_key, 
                          base_url: str = "https://api.anthropic.com/v1", 
                          model: str = "claude-sonnet-4-5-20250929", 
                          max_tokens: int = 10240, 
                          temperature: float = 0.0, 
                          json_schema: dict = None) -> Generator[Tuple[str, Optional[str]], None, None]:
 
    
    url = f"{base_url}/messages"
    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01"
    }
    
    # æž„å»º payload
    if json_schema:
        user_content = f"{prompt}\n\nIMPORTANT: Output ONLY valid JSON in this exact format, with no markdown code blocks, no explanations, no extra text:\n{str(json_schema)}\n\nData to process:\n{article}"
    else:
        user_content = f"{prompt}\n\n{article}"
    
    
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": user_content}],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": True  # æµå¼ä¼ è¾“
    }

    full_response = ""  # ðŸ”‘ æ”¶é›†å®Œæ•´å†…å®¹
    
    try:
        response = requests.post(url, headers=headers, json=payload, stream=True)
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                line_text = line.decode('utf-8')
                
                if not line_text.startswith('data: '):
                    continue
                
                json_str = line_text[6:]
                
                if json_str == '[DONE]':
                    break
                
                try:
                    event_data = json.loads(json_str)
                    
                    if event_data.get('type') == 'content_block_delta':
                        delta = event_data.get('delta', {})
                        if delta.get('type') == 'text_delta':
                            text_chunk = delta.get('text', '')
                            if text_chunk:
                                full_response += text_chunk  # ç´¯ç§¯å®Œæ•´å†…å®¹
                                yield (text_chunk, None)  # ðŸ”‘ æµå¼è¿”å›žç‰‡æ®µï¼Œå®Œæ•´å†…å®¹ä¸º None
                                
                except json.JSONDecodeError:
                    continue
        
       
        if full_response:
            if json_schema:
                # æ¸…ç† JSON
                json_match = re.search(r'```(?:json)?\s*\n(.*?)\n```', full_response, re.DOTALL)
                if json_match:
                    json_text = json_match.group(1).strip()
                else:
                    json_text = full_response.strip()
                
                json_text = re.sub(r'\\(?!["\\/bfnrtu])', r'\\\\', json_text)
                yield ("", json_text)  # ðŸ”‘ æœ€åŽè¿”å›žå®Œæ•´ JSON
            else:
                yield ("", full_response)  # ðŸ”‘ æœ€åŽè¿”å›žå®Œæ•´æ–‡æœ¬
        else:
            yield ("", None)
                    
    except requests.exceptions.RequestException as e:
        print(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"APIé”™è¯¯å“åº”: {e.response.text}")
        yield ("", None)



def query_openai_model(prompt: str, article: str, api_key: str=openai_key, base_url: str = "https://api.openai.com/v1", 
                       model: str = "gpt-5-chat-latest", max_tokens: int = 10240, 
                       temperature: float = 0.8,json_schema: dict = None) -> Optional[str]:
    
    url = f"{base_url}/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": f"{prompt}\n \n{article}"}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    if json_schema:
        payload["response_format"] = {
            "type": "json_schema",
            "json_schema": json_schema
        }
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        response_json = response.json()
        if "choices" in response_json and len(response_json["choices"]) > 0:
            text_content = response_json["choices"][0]["message"]["content"]
            return text_content
        else:
            print("APIè¿”å›žå†…å®¹æ ¼å¼å¼‚å¸¸")
            return None
    except requests.exceptions.RequestException as e:
        print(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"APIé”™è¯¯å“åº”: {e.response.text}")
        return None
    


def search_news(tavily_api_key: str=tavily_key, query: str="", max_results: int = 15, days: int = 3, 
                search_depth: str = "basic", include_answer: bool = True) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨Tavily APIæœç´¢æ–°é—»
    
    Args:
        tavily_api_key: Tavily APIå¯†é’¥
        query: æœç´¢æŸ¥è¯¢è¯
        max_results: æœ€å¤§è¿”å›žç»“æžœæ•° (é»˜è®¤10)
        days: æœç´¢æ—¶é—´èŒƒå›´(å¤©) (é»˜è®¤7å¤©)
        search_depth: æœç´¢æ·±åº¦ "basic" æˆ– "advanced" (é»˜è®¤basic)
        include_answer: æ˜¯å¦åŒ…å«AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦ (é»˜è®¤True)
    
    Returns:
        åŒ…å«æœç´¢ç»“æžœçš„å­—å…¸åˆ—è¡¨
    """
    url = "https://api.tavily.com/search"
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {tavily_api_key}'
    }
    
    data = {
        "query": query,
        "topic": "news",  # ä¸“é—¨æœç´¢æ–°é—»
        "search_depth": search_depth,  # æœç´¢æ·±åº¦
        "chunks_per_source": 3,  # æ¯ä¸ªæ¥æºçš„å†…å®¹å—æ•°
        "max_results": max_results,  # æœ€å¤§ç»“æžœæ•°
        "time_range": None,  # æ—¶é—´èŒƒå›´(nullè¡¨ç¤ºä½¿ç”¨dayså‚æ•°)
        "days": days,  # æœç´¢æœ€è¿‘Nå¤©çš„æ–°é—»
        "include_answer": include_answer,  # åŒ…å«AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦
        "include_raw_content": False,  # ä¸åŒ…å«åŽŸå§‹HTMLå†…å®¹
        "include_images": False,  # åŒ…å«å›¾ç‰‡
        "include_image_descriptions": False,  # ä¸åŒ…å«å›¾ç‰‡æè¿°
        "include_domains": [],  # åŒ…å«çš„åŸŸååˆ—è¡¨(ç©ºè¡¨ç¤ºä¸é™åˆ¶)
        "exclude_domains": [],  # æŽ’é™¤çš„åŸŸååˆ—è¡¨
        "country": None  # å›½å®¶é™åˆ¶(nullè¡¨ç¤ºå…¨çƒ)
    }
    
    try:
        print(f"æ­£åœ¨æœç´¢æ–°é—»: {query}")
        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        
        result = response.json()
        
        # æ£€æŸ¥APIå“åº”çŠ¶æ€
        if 'results' not in result:
            print(f"âš ï¸ APIå“åº”å¼‚å¸¸: {result}")
            return []
        
        results = result.get("results", [])
        answer = result.get("answer", "")  # AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦
        
        print(f"æ‰¾åˆ° {len(results)} æ¡æ–°é—»ç»“æžœ\n")
        # print(result)
        
        
        # å¦‚æžœåŒ…å«AIç­”æ¡ˆæ‘˜è¦ï¼Œæ‰“å°å‡ºæ¥
        if include_answer and answer:
            pass
            # print(f"AIæ‘˜è¦: {answer[:]}...")
        
        return answer, results
        
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿žæŽ¥")
        return []
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTPé”™è¯¯: {e}")
        if hasattr(e.response, 'text'):
            print(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
        return []
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚é”™è¯¯: {e}")
        return []
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return []



def chose_keywords(keyword_list, n=1):
   
    if n >= len(keyword_list):
        return keyword_list
    
    max_rank = max(item.get('real_time_rank', 20) for item in keyword_list)
    
   
    weighted_items = []
    for item in keyword_list:
        rank = item.get('real_time_rank', 20)
        weight = (max_rank + 1) - rank
        weighted_items.append((item, weight))
    
  
    weighted_pool = []
    for item, weight in weighted_items:
        weighted_pool.extend([item] * weight)
    
    
    selected = []
    remaining_pool = weighted_pool.copy()
    
    while len(selected) < n and remaining_pool:
        chosen = random.choice(remaining_pool)
        if chosen not in selected:
            selected.append(chosen)
       
        remaining_pool = [item for item in remaining_pool if item != chosen]
    
    return selected

def get_news_seo_articles(keywords:str =None,company_describe=None):
    from news_seo_prompts import news_keywords,news_search_prompt,news_schema,extract_prompt,rewrite_prompt,seo_keywords,seo_metadata,seo_rewrite_prompt 
    date_str = datetime.now().strftime("%Y%m%d")
    news_pool=[]


    if keywords:
        keyword_str=keywords
        all_keyword=keywords
        category=""
        
        ai_summary,news_results=search_news(query=news_search_prompt.format(category=category,keyword=keyword_str))
        print(ai_summary)
        yield f"**News Search Results**\n{ai_summary}\n\n"
        all_news_results = news_results  
        
    else:
        
        keywords_list=chose_keywords(news_keywords,n=1)
        all_news_results = [] 
        all_keyword=""
        for i, news in enumerate(keywords_list): 
            category = news.get("category", "")
            keyword_str = news.get("keyword_en", "")
            all_keyword +=keyword_str 
            ai_summary,news_result=search_news(query=news_search_prompt.format(category=category,keyword=keyword_str))
            yield f"**News Search Results**\n{ai_summary}\n\n"
            print(ai_summary)
            
            # print(news_result)
            all_news_results.extend(news_result) 

  
    print("æ­£åœ¨æ•´ç†æ–°é—»")
    yield "**ðŸ“° Organizing news...**\n\n"
    for chunk, complete in query_gpt_model(prompt=extract_prompt.format(date_str=date_str,keywords=all_keyword),article=str(all_news_results),json_schema=news_schema):
        
        if chunk:
            yield chunk
            print(chunk, end='', flush=True)  # å®žæ—¶æ˜¾ç¤ºç»™ç”¨æˆ·
        if complete is not None:
            extract_news = complete  # æ”¶é›†å®Œæ•´ç»“æžœ
    # print(extract_news)
    extract_news=json.loads(extract_news)
    news_pool.append(extract_news["news_list"])

    #åŽ»å¯¹news_poolè¿›è¡ŒåŽ»é‡
    all_news = []
    for news_list in news_pool:
        all_news.extend(news_list)

    # ç”¨å­—å…¸åŽ»é‡ï¼ŒURLä½œä¸ºkey
    unique_news_dict = {}
    for news in all_news:
        url = news.get('url')
        if url:
            unique_news_dict[url] = news  # å¦‚æžœURLé‡å¤ï¼Œä¼šè¢«è¦†ç›–

    news_pool = list(unique_news_dict.values())
   


    # for chunk, complete in query_gpt_model(prompt=rewrite_prompt.format(str(news_pool)),article=""):
    
    #     if chunk:
    #         print(chunk, end='', flush=True)  # å®žæ—¶æ˜¾ç¤ºç»™ç”¨æˆ·
    #     if complete is not None:
    #         rewritten_article = complete  # æ”¶é›†å®Œæ•´ç»“æžœ

    print("æ™ºèƒ½åˆ†æžå…³é”®è¯")
    yield "\n\n**ðŸ” Analyzing keywords intelligently...**\n\n"
    for chunk, complete in query_gpt_model(prompt=seo_keywords.format(company=company_describe,news=str(news_pool)),article=""):
        
        if chunk:
            yield chunk
            print(chunk, end='', flush=True)  # å®žæ—¶æ˜¾ç¤ºç»™ç”¨æˆ·
        if complete is not None:
            extract_keywords = complete  # æ”¶é›†å®Œæ•´ç»“æžœ
    print("æž„å»ºæ–‡ç« ç»“æž„")
    yield "\n\n**ðŸ“‹ Building article structure...**\n\n"
    for chunk, complete in query_gpt_model(prompt=seo_metadata.format(company=company_describe,keywords=extract_keywords,news=str(news_pool)),article=""):
       
        if chunk:
            yield chunk
            print(chunk, end='', flush=True)  # å®žæ—¶æ˜¾ç¤ºç»™ç”¨æˆ·
        if complete is not None:
            metadata = complete  # æ”¶é›†å®Œæ•´ç»“æžœ
    print("ä¸“ä¸šseoæ–‡ç« ç”Ÿæˆä¸­")
    yield "\n\n**âœï¸ Generating professional SEO article...**\n\n"
    for chunk, complete in query_gpt_model(prompt=seo_rewrite_prompt.format(news=str(news_pool),keywords=extract_keywords,metadata=metadata),article=""):
       
        if chunk:
            yield chunk
            print(chunk, end='', flush=True)  # å®žæ—¶æ˜¾ç¤ºç»™ç”¨æˆ·
        if complete is not None:
            seo_article = complete  # æ”¶é›†å®Œæ•´ç»“æžœ









if __name__ == "__main__":
#   
    get_news_seo_articles( keywords="what about Beijing private jet")